# Kubeadm for K8s clustser creation, with packer and ansible to create the node AMI image on AWS2 us-east-1:

#




# Terraform to Kubeadm
This repository hosts the code that was demonstrated in this video https://youtu.be/F-ZJYjqr00Y
## The kubeadm config file
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  extraArgs:
    cloud-provider: aws
clusterName: # PLACE YOUR CLUSTER NAME HERE 
controlPlaneEndpoint: # PLACE THE URL TO THE API LOAD BALANCER HERE
controllerManager:
  extraArgs:
    cloud-provider: aws
    configure-cloud-routes: "false"
kubernetesVersion: stable
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12

# Pakcer node.pkr.hcl to create the AWS AMI image in accordance with the playbook.yaml ansible file below
# NOTE that bash scripts can be used instead of Ansible and also Puppet or Chef are alternatives as well
# The source image is ubuntu 20.04

packer {
  required_plugins {
    amazon = {
      source  = "github.com/hashicorp/amazon"
      version = "~> 1.2.1"
    }
    ansible = {
      version = "~> 1"
      source = "github.com/hashicorp/ansible"
    }
  }
// https://developer.hashicorp.com/packer/integrations
}

source "amazon-ebs" "ubuntu" {
// amazon ami image based on ubuntu
  ami_name      = "k8s-node"
  instance_type = "t3.micro"
  //region        = "eu-west-2"
  region        = "us-east-1"
  //source_ami base http://cloud-images.ubuntu.com/locator/ec2/
  source_ami_filter {
    filters = {
      name                = "ubuntu/images/*ubuntu-focal-20.04-amd64-server-*"
      root-device-type    = "ebs"
      virtualization-type = "hvm"
    }
    most_recent = true
    //cannanocal
    owners      = ["099720109477"]
  }
  ssh_username = "ubuntu"
  force_deregister = true
  force_delete_snapshot = true
}

build {
  name    = "k8s-node"
  sources = [
    "source.amazon-ebs.ubuntu"
  ]
  provisioner "ansible" {
    playbook_file = "./ansible/playbook.yaml"
    //playbook file provided.
  }
}





#BUILD: Ansible playbook.yaml updated with latest repo create the node image for mastser and worker nodes docker, kubectl
# kubeadm and kubelet installed. 

- name: "Provision a k8s image"
  hosts: default
  # packer needs to access this playbook to provision the hosts (master ndoes and worker nodes in the k8s cluster)
  become: true
  # commands need to be run as root

  tasks:
  # Kubernetes components
  ## Docker. First install docker itself on the node
  - apt:
     name: ['apt-transport-https', 'ca-certificates', 'curl', 'gnupg-agent', 'software-properties-common']
     update_cache: yes
  - apt_key:
     url: https://download.docker.com/linux/ubuntu/gpg
     state: present
  - apt_repository:
     repo: "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
     state: present
  - apt:
      name: ["docker-ce", "docker-ce-cli", "containerd.io"]
      state: present
      update_cache: yes
  - name: Enable the Docker service and ensure that it starts with system boot
    systemd:
      name: docker
      enabled: yes
      state: started
  - name: Add the Kubernetes repository key
    apt_key:
      ##url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
      url: https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key
      state: present
  - name: Add the Kubernetes repository
    apt_repository:
      # repos deprecated: https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/
      # updated repo: https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/
      #repo: "deb https://apt.kubernetes.io/ kubernetes-xenial main"
      repo: "deb https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /"
      state: present
  - name: Install the components
    apt:
      name: ["kubelet","kubeadm","kubectl"]
      update_cache: yes

  - name: Configure kubelet to use cgroupfs
    ansible.builtin.lineinfile:
      path: /etc/default/kubelet
      line: 'KUBELET_EXTRA_ARGS="--cgroup-driver=cgroupfs"'
      create: yes

   # Preparing the instance for Kubernetes (kernel parameters)
  - name: "adding sysctl net.ipv4.ip_forward"
    sysctl:
      name: net.ipv4.ip_forward
      value: '1'
      state: present
      reload: yes

  - name: "adding sysctl net.bridge.bridge-nf-call-iptables"
    sysctl:
      name: net.bridge.bridge-nf-call-iptables
      value: '1'
      state: present
      reload: yes



# Command syntax to create the AMI image on AWS in us-east-1
# IMPORTANT NOTE: I am using an EC2 instance as the ansible/packer controller and terraform controller and not the local mac
# All commands below executed on the ansible ubuntu 22 ansible controller with packer and aws v2.0 and terraform installed (along with python3
# and boto3 and botocore)

ubuntu@ip-172-31-31-120:~/course3_kubeadm/kubernetes-ami$ packer init .

ubuntu@ip-172-31-31-120:~/course3_kubeadm/kubernetes-ami$ packer build node.pkr.hcl 

node.pkr.hcl calls the ansible playbook under the ansbile/playbook.yaml folder

ubuntu@ip-172-31-31-120:~/course3_kubeadm/kubernetes-ami$ ls
ansible  node.pkr.hcl
ubuntu@ip-172-31-31-120:~/course3_kubeadm/kubernetes-ami$ pwd
/home/ubuntu/course3_kubeadm/kubernetes-ami


The ami-id is us-east-1: ami-0704ecf0bdb63417e


# The AMI image (see above)
For this setup to work you are expected to use an AMI that contains the following:
* Docker
* kuebctl
* kubeadm
* kubelet



The versions of the above software depends on the version of Kubernnetes that you intend to use. The code in this lab automatically 
selects the latest version of Kubernetes.


# Terraform administration of the kubernetes master control nodes and worker nodes

This is done on the ansible packer controller EC2 instance
The folder structure for the terraform is in the Resources_terraform directory

ubuntu@ip-172-31-31-120:~/course3_kubeadm$ mkdir Resources_terraform
ubuntu@ip-172-31-31-120:~/course3_kubeadm$ ls -la
total 16
drwxrwxr-x  4 ubuntu ubuntu 4096 Jun  7 23:41 .
drwxr-x--- 16 ubuntu ubuntu 4096 Jun  7 22:35 ..
drwxrwxr-x  2 ubuntu ubuntu 4096 Jun  7 23:41 Resources_terraform
drwxrwxr-x  3 ubuntu ubuntu 4096 Jun  7 21:39 kubernetes-ami

In the Resources_terraform directory there is the kubeadm.config file which will be used to administer the cluster from
the master node (see below)

In the Resources_terraform directory there is a folder called terraform_kubeadm that has the terraform main.tf and kubernetes folder
with the rest of the terraform module files

In the Resources_terraform directory go into the terraform_kubeadm directory
The main.tf file is there. Simply, run `terraform init` followed by `terraform plan` and  `terraform apply`. 

Note that the EC2 controller has a role attached to it for Adminstrator access to the AWS account.  The priveleges
must be at least enough to provison the infra in the terraform code

Once the infrastructure is up and running, use the jump (bastion) server to login to the master node and execute `kubeadm init --config kubeadm.confg` 
where an example`kubeadm.config` file is listed above. The `kubeadm` command will print the necessary join command for other worker (or master) nodes. 
Copy and execute the command as necessary. The KUBECONFIG file is always created in `/etc/kubernetes/admin.conf`. 
You can use this file to create users and workflows.